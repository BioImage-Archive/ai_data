import json
import os
import pandas as pd
import pathlib
import os
import yaml

# Read the input JSON file
# with open("metadata.json") as f:
#     metadata = json.load(f)

# import logging
# logger = logging.getLogger(__name__)
# All of this should be in a config file in this dir really
metadata_dir = "metadata"
projects_metadata = os.listdir(metadata_dir)
meta_project_name = "ZeroCostDL4Mic"
metadata_files = [
    "metadata.tsv",
    "file_list.tsv",
    "annotations_list.tsv",
    "pagetab.tsv",
]


def load_yaml(file):
    with open(file) as f:
        return yaml.safe_load(f)


rule all:
    wildcard_constraints:
        meta_project="[^/]*",
        project="[^/]*",
        folder=".*",
        filename="[^/.]*",
        ext="[^.]*",
    input:
        expand(
            "{meta_project}/{project}/{metadata_files}",
            meta_project=meta_project_name,
            project=projects_metadata,
            metadata_files=metadata_files,
        ),
        # expand("{project}/file_list.yaml",project=projects_metadata),
        # Download_and_extract 
        expand("data/{project}", project=projects_metadata),
        expand(
            "{meta_project}/{project}/pagetab.tsv",
            meta_project=meta_project_name,
            project=projects_metadata,
        ),


rule:
    wildcard_constraints:
        meta_project="[^/]*",
        project="[^/]*",
        folder=".*",
        filename="[^/.]*",
        ext="[^.]*",
    input:
        agent_config=metadata_dir + "/{project}/bia.yaml",
        accession = metadata_dir + "/{project}/accession",
    output:
        pagetab="{meta_project}/{project}/pagetab.tsv",
    params:
        accession=lambda wildcards, input: pathlib.Path(input.accession).read_text(),
    shell:
        """
        bia-agent rembi-to-pagetab "{input.agent_config}" > "{output.pagetab}" {params.accession}
        """


checkpoint download_and_extract:
    wildcard_constraints:
        meta_project="[^/]*",
        project="[^/]*",
        folder=".*",
        filename="[^/.]*",
        ext="[^.]*",
    input:
        mapper=metadata_dir + "/{project}/file_list.yaml",
    output:
        folder=directory("data/{project,[^/]+}"),
        archive="{project,[^/]+}.zip",
    params:
        url=lambda wildcards, input: load_yaml(input.mapper)["url"],
    shell:
        """
        wget "{params.url}" -O "{output.archive}"
        mkdir -p "{output.folder}"
        unzip "{output.archive}" -d "{output.folder}"
        """


def get_jsons(wildcards):
    checkpoint_output = checkpoints.download_and_extract.get(**wildcards).output
    glob_string = (
        f"data/{wildcards.project}/{{folder,.*}}/{{filename,[^/.][^/]*}}.{{ext,[^.]+}}"
    )
    folders, filenames, ext = glob_wildcards(glob_string)
    file_list = expand(
        f"{wildcards.meta_project}/{wildcards.project}/{{folder}}/{{filename}}.{{ext}}.metadata.json",
        zip,
        project=wildcards.project,
        folder=folders,
        filename=filenames,
        ext=ext,
        allow_missing=True,
    )

    # breakpoint()

    return file_list


rule save_file_list:
    wildcard_constraints:
        meta_project="[^/]*",
        project="[^/]*",
        folder=".*",
        filename="[^/.]*",
        ext="[^.]*",
    output:
        json="{meta_project}/{project}/metadata_list.json",
    run:
        file_list = get_jsons(wildcards)
        if file_list == []:
            raise Exception("No files found")
            # breakpoint()
        with open(output.json, "w") as f:
            json.dump(file_list, f, indent=4)
            # shell(f"echo '{file_list}' > {output.json_file}")



def get_metadata(wildcards):
    file_list_json = f"{wildcards.meta_project}/{wildcards.project}/file_list.json"
    with open(file_list_json) as f:
        file_list = json.load(f)
    return file_list


def transform_row_to_dict(row):
    # Create a dictionary with the desired structure
    transformed_dict = {
        "path": row.name,  # Assuming the index of the DataFrame is the path
        "attributes": [],
        "type": "file",
    }

    # Add the column names and their corresponding values to the "attributes" list
    for column_name, column_value in row.items():
        attribute = {"name": column_name, "value": column_value}
        transformed_dict["attributes"].append(attribute)

    return transformed_dict


# Make this into function and call with shell
checkpoint collate_json:
    wildcard_constraints:
        meta_project="[^/]*",
        project="[^/]*",
        folder=".*",
        filename="[^/.]*",
        ext="[^.]*",
    input:
        json=get_jsons,
    output:
        # json="{meta_project}/{project}/metadata.json",
        metadata_tsv="{meta_project}/{project}/metadata.tsv",
        file_list_tsv="{meta_project}/{project}/file_list.tsv",
        annotations_list_tsv="{meta_project}/{project}/annotations_list.tsv",
    run:
        # Create an empty list to store individual DataFrames
        dfs = []
        for json_file in input.json:
            # Read JSON into a DataFrame
            df = pd.read_json(json_file, orient="index").T
            # Set the name of the index as the filepath
            # I think this needs to be smarter
            df["Files"] = json_file.replace(".metadata.json", "")
            df.set_index("Files", inplace=True)

            dfs.append(df)

        final_df = pd.concat(dfs)

        annotations_list_tsv = final_df[final_df["source image"] == ""]
        file_list_tsv = final_df.drop(annotations_list_tsv.index)
        mapper = {
            output.metadata_tsv: final_df,
            output.file_list_tsv: file_list_tsv,
            output.annotations_list_tsv: annotations_list_tsv,
        }
        mapper.items()
        for file_name, df_to_save in mapper.items():
            df_to_save.to_csv(file_name, sep="\t")


rule copy_image:
    input:
        image="data/{project}/{folder}/{filename}.{ext}",
    wildcard_constraints:
        meta_project="[^/]*",
        project="[^/]*",
        folder=".*",
        filename="[^/.]*",
        ext="[^.]*",
    output:
        image="{meta_project}/{project}/{folder}/{filename}.{ext}",
    shell:
        """
    ln "{input.image}" "{output.image}"
    """

# TODO make this into a function and call with shell
rule process_image:
    wildcard_constraints:
        meta_project="[^/]*",
        project="[^/]*",
        folder=".*",
        filename="[^/.]*",
        ext="[^.]*",
    input:
        cp="{meta_project}/{project}/{folder}/{filename}.{ext}",
        filename="data/{project}/{folder}/{filename}.{ext}",
        metadata=metadata_dir+"/{project}/file_list.yaml",
        # filename = "{meta_project}/{project}/{folder}/{filename}.{ext}"
    output:
        json="{meta_project}/{project}/{folder}/{filename}.{ext}.metadata.json",
    params:
        # metadata=lambda input, wildcards: load_yaml(f"metadata/{wildcards.project}/file_list.yaml"),
        metadata=lambda wildcards, input: load_yaml(input.metadata),
    run:
        # breakpoint()
        # Check if the metadata key exists
        metadata = params.metadata
        metadata_key = params.metadata.get("data", {}).get(
            wildcards.project + "/" + wildcards.folder, {}
        )
        if not metadata_key:
            print(f"skip {wildcards.project}/{wildcards.folder}")
        else:
            # Check if 'source image' exists in metadata_key
            if "source image" in metadata_key:
                if metadata_key["source image"] != "":
                    source_image = f"{wildcards.meta_project}/{wildcards.project}/{metadata_key['source image']}/{wildcards.filename}.{wildcards.ext}"
                    # breakpoint()
                    metadata_key["source image"] = source_image

        metadata_dict = metadata_key

        with open(output.json, "w") as f:
            json.dump(metadata_dict, f, indent=4)
