import json
import os
import pandas as pd
import pathlib

# Read the input JSON file
with open("metadata.json") as f:
    metadata = json.load(f)

projects_metadata = list(metadata.keys())
meta_project_name = "ZeroCostDL4Mic"
print(projects_metadata)
rule all:
    input:
        expand("{meta_project}/{project}/metadata.json",meta_project=meta_project_name,
        project=projects_metadata),
 
checkpoint download_and_extract:
    input:
        metadata_json="metadata.json",
    output:
        folder=directory("data/{project,[^/]+}"),
        archive="{project,[^/]+}.zip",
    params:
        url=lambda wildcards, input: metadata[wildcards.project]["url"],
    shell:
        """
        wget "{params.url}" -O "{output.archive}"
        mkdir -p "{output.folder}"
        unzip "{output.archive}" -d "{output.folder}"
        """

def get_jsons(wildcards):
    # breakpoint()
    checkpoint_output = checkpoints.download_and_extract.get(**wildcards).output
    folders, filenames, ext = glob_wildcards(
        f"data/{wildcards.project}/{{folder}}/{{filename,[^/.]+}}.{{ext,[^/.]+}}"
    )
    file_list = expand(
        f"{wildcards.meta_project}/{wildcards.project}/{{folder}}/{{filename}}.{{ext}}.metadata.json",
        zip,
        # project=wildcards.project,
        folder=folders,
        filename=filenames,
        ext=ext,
        allow_missing=True,
    )
    return file_list


checkpoint collate_json:
    input:
        json=get_jsons,
    output:
        json="{meta_project,[^/]+}/{project,[^/]+}/metadata.json",
        csv="{meta_project,[^/]+}/{project,[^/]+}/metadata.csv",
    run:
        # Create an empty list to store individual DataFrames
        dfs = []
        # Iterate over the input JSON files
        for json_file in input.json:
            # breakpoint()
            # Read JSON into a DataFrame
            df = pd.read_json(json_file, orient="index").T

            # Set the name of the index as the filepath
            df["filepath"] = json_file
            df.set_index("filepath", inplace=True)

            # Append the DataFrame to the list
            dfs.append(df)

            # Concatenate all the DataFrames
        # breakpoint()
        final_df = pd.concat(dfs)
        final_df.to_json(output.json, orient="index", indent=4)
        final_df.to_csv(output.csv)


rule copy_image:
  input:
    image="data/{project,[^/]+}/{folder}/{filename,[^/.]+}.{ext,[^.]+}",
  output:
    image="{meta_project,[^/]+}/{project,[^/]+}/{folder}/{filename,[^/.]+}.{ext,[^.]+}"
  shell:
    """
    ln "{input.image}" "{output.image}"
    """

rule process_image:
    input:
        # filename = "data/{project,[^/]+}/{folder}/{filename,[^/]+}.{ext}",
        filename = "{meta_project,[^/]+}/{project,[^/]+}/{folder}/{filename,[^/.]+}.{ext,[^.]+}"
    output:
        json="{meta_project,[^/]+}/{project,[^/]+}/{folder}/{filename,[^/.]+}.{ext,[^.]+}.metadata.json",
    run:
        # breakpoint()
        path = pathlib.Path(input.filename)
        parent_dir = path.parent
        full_path = f"{wildcards.meta_project}/{wildcards.project}/{wildcards.folder}/{wildcards.filename}.{wildcards.ext}"

        # Check if the metadata key exists
        metadata_key = metadata[wildcards.project].get("data", {}).get(wildcards.project+"/"+wildcards.folder, {})

        if not metadata_key:
            print(f"skip {wildcards.project}/{wildcards.folder}")
        else:
            # Check if 'source image' exists in metadata_key
            if 'source image' in metadata_key:
                source_image = f"{wildcards.project}/{metadata_key['source image']}/{wildcards.filename}.{wildcards.ext}"
                metadata_key["source image"] = source_image 

        metadata_dict = metadata_key

        with open(output.json, "w") as f:
            json.dump(metadata_dict, f, indent=4)
