{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  %%\n",
        "import sys\n",
        "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
        "from pyro.optim import Adam\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "import pyro.distributions as dist\n",
        "import pyro\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import glob\n",
        "# Note - you must have torchvision installed for this example\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "from skimage.measure import regionprops\n",
        "from torchvision.transforms.functional import crop\n",
        "from scipy import ndimage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "import torchvision\n",
        "\n",
        "path = os.path.join(os.path.expanduser(\"~\"),\n",
        "                    \"data-science-bowl-2018/stage1_train/*/masks/*.png\")\n",
        "path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "window_size = 128-32\n",
        "\n",
        "class cropCentroid(torch.nn.Module):\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, img):\n",
        "        return self.crop_centroid(img, self.size)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f\"(size={self.size})\"\n",
        "\n",
        "    def crop_centroid(self, image, size):\n",
        "        np_image = np.array(image)\n",
        "        im_height, im_width = np_image.shape\n",
        "\n",
        "        properties = regionprops(np_image.astype(int),\n",
        "                                 np_image.astype(int))\n",
        "        center_of_mass = properties[0].centroid\n",
        "        # weighted_center_of_mass = properties[0].weighted_centroid\n",
        "        top = int(center_of_mass[0]-size/2)\n",
        "        left = int(center_of_mass[1]-size/2)\n",
        "        height, width = size, size\n",
        "        # TODO find bad croppings\n",
        "        # if ((top <= 0)  or (top+height >= im_height)  or (left <= 0) or (left+width >= 0) ):\n",
        "        # return Image.eval(crop(image,top,left,height,width), (lambda x: 0))\n",
        "        return crop(image, top, left, height, width)\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        cropCentroid(window_size),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize(0, 1),\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor()\n",
        "        # transforms.RandomCrop((512, 512)),\n",
        "        # transforms.ConvertImageDtype(torch.bool)\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "class DSB2018(Dataset):\n",
        "    def __init__(self, path_glob, transform=None):\n",
        "        self.image_paths = glob.glob(path_glob, recursive=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = Image.open(self.image_paths[index])\n",
        "        # if self.transform is not None:\n",
        "        x = self.transform(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "\n",
        "train_dataset_glob = os.path.join(os.path.expanduser(\"~\"),\n",
        "                                  \"data-science-bowl-2018/stage1_train/*/masks/*.png\")\n",
        "# test_dataloader_glob=os.path.join(os.path.expanduser(\"~\"),\n",
        "# \"data-science-bowl-2018/stage1_test/*/masks/*.png\")\n",
        "train_dataset = DSB2018(train_dataset_glob, transform=transform)\n",
        "train_dataset[0]\n",
        "#  %%\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                        shuffle=True, num_workers=8, pin_memory=True)\n",
        "\n",
        "#  %%\n",
        "\n",
        "# fig,ax = plt.subplots(10,10)\n",
        "# for i,ax in enumerate(ax.flat):\n",
        "#     ax.imshow(transform(train_dataset[i]).reshape(window_size,window_size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#  %%\n",
        "\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        z_dim = (5, 5)\n",
        "        decoder_input = (12, 12)\n",
        "        self.conv1 = self.contract_block(in_channels, 32, 7, 3)\n",
        "        self.conv2 = self.contract_block(32, 64, 3, 1)\n",
        "        self.conv3 = self.contract_block(64, 1, 3, 1)\n",
        "\n",
        "        # self.softmax = nn.AdaptiveSoftmax()\n",
        "        self.fc1 = nn.AdaptiveMaxPool2d(z_dim)\n",
        "        self.fc2 = nn.AdaptiveMaxPool2d(decoder_input)\n",
        "\n",
        "        # self.conv4 = self.contract_block(128, 256, 3, 1)\n",
        "\n",
        "        # self.upconv4 = self.contract_block(256, 128, 3, 1)\n",
        "        self.upconv3 = self.expand_block(1, 64, 3, 1)\n",
        "        self.upconv2 = self.expand_block(64 * 1, 32, 3, 1)\n",
        "        self.upconv1 = self.expand_block(32 * 1, out_channels, 3, 1)\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            self.conv1, self.conv2, self.conv3, self.fc1)\n",
        "        self.decoder = nn.Sequential(\n",
        "            self.fc2, self.upconv3, self.upconv2, self.upconv1)\n",
        "\n",
        "    # Call is essentially the same as running \"forward\"\n",
        "    def __call__(self, x):\n",
        "        # x = self.encoder(x)\n",
        "        # x = self.decoder(x)\n",
        "        return self.forward(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
        "\n",
        "        contract = nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=1,\n",
        "                padding=padding,\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(out_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(\n",
        "                out_channels,\n",
        "                out_channels,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=1,\n",
        "                padding=padding,\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(out_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "        return contract\n",
        "\n",
        "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
        "\n",
        "        expand = nn.Sequential(\n",
        "            torch.nn.Conv2d(\n",
        "                in_channels, out_channels, kernel_size, stride=1, padding=padding\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(out_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(\n",
        "                out_channels, out_channels, kernel_size, stride=1, padding=padding\n",
        "            ),\n",
        "            torch.nn.BatchNorm2d(out_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.ConvTranspose2d(\n",
        "                out_channels,\n",
        "                out_channels,\n",
        "                kernel_size=3,\n",
        "                stride=2,\n",
        "                padding=1,\n",
        "                output_padding=1,\n",
        "            ),\n",
        "        )\n",
        "        return expand\n",
        "\n",
        "\n",
        "model = AutoEncoder(1, 1)\n",
        "img = train_dataset[0].unsqueeze(0)\n",
        "y = model(img)\n",
        "z = model.encoder(img)\n",
        "print(f\"img_dims:{img.shape} y:_dims:{y.shape} z:_dims:{z.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#  %% VAE\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    # by default our latent space is 50-dimensional\n",
        "    # and we use 400 hidden units\n",
        "    def __init__(self, h_dim=(1, 5, 5), z_dim=(1, 5, 5), use_cuda=False):\n",
        "        super().__init__()\n",
        "        # create the encoder and decoder networks\n",
        "        self.autoencoder = AutoEncoder(1, 1)\n",
        "        self.encoder = self.autoencoder.encoder\n",
        "        self.decoder = self.autoencoder.decoder\n",
        "\n",
        "        self.z_dim = torch.tensor(z_dim)\n",
        "        self.x_dim = (1, window_size, window_size)\n",
        "        self.h_dim = torch.tensor(h_dim)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.fc1 = nn.Linear(torch.prod(self.h_dim), torch.prod(self.z_dim))\n",
        "        self.fc21 = nn.Linear(torch.prod(self.h_dim), torch.prod(self.z_dim))\n",
        "        self.fc22 = nn.Linear(torch.prod(self.h_dim), torch.prod(self.z_dim))\n",
        "\n",
        "        pyro.module(\"decoder\", self.autoencoder)\n",
        "\n",
        "        # self.fc3 = nn.Linear(torch.prod(self.z_dim), torch.prod(self.h_dim))\n",
        "        self.softplus = nn.Softplus()\n",
        "    \n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        # h = self.softplus(h)\n",
        "        # h = self.flatten(h)\n",
        "        # z = self.sigmoid(h)\n",
        "        \n",
        "        # No clue if this is actually mu\n",
        "        z = self.fc21(self.flatten(h))\n",
        "        mu = torch.exp(self.fc22(self.flatten(h)))\n",
        "        # z, mu = self.bottleneck(h)\n",
        "        return z.reshape(h.shape), mu.reshape(h.shape)\n",
        "\n",
        "    def decode(self, z):\n",
        "        # z = self.fc3(z).reshape(-1,*tuple(self.h_dim))\n",
        "        z = z.reshape(-1, *tuple(self.h_dim))\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z, mu = self.encode(x)\n",
        "        return self.decode(z)\n",
        "        \n",
        "\n",
        "    def model(self, x):\n",
        "        # register PyTorch module `decoder` with Pyro\n",
        "        pyro.module(\"decoder\", self.decoder)\n",
        "        with pyro.plate(\"data\", x.shape[0]):\n",
        "            # setup hyperparameters for prior p(z)\n",
        "            z_loc = x.new_zeros(torch.Size((x.shape[0], *tuple(self.h_dim))))\n",
        "            z_scale = x.new_ones(torch.Size((x.shape[0], *tuple(self.h_dim))))\n",
        "            # sample from prior (value will be sampled by guide when computing the ELBO)\n",
        "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(3))\n",
        "            # decode the latent code z\n",
        "            loc_img = torch.sigmoid(self.decode(z))\n",
        "            # score against actual images\n",
        "            pyro.sample(\"obs\", dist.Bernoulli(loc_img).to_event(3), obs=x)\n",
        "\n",
        "    # define the guide (i.e. variational distribution) q(z|x)\n",
        "    def guide(self, x):\n",
        "        # register PyTorch module `encoder` with Pyro\n",
        "        pyro.module(\"encoder\", self.encoder)\n",
        "        with pyro.plate(\"data\", x.shape[0]):\n",
        "            # use the encoder to get the parameters used to define q(z|x)\n",
        "            z_loc, z_scale = self.encode(x)\n",
        "            # sample the latent code z\n",
        "            pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(3))\n",
        "\n",
        "    def construct_from_z(self,z):\n",
        "        return torch.sigmoid(self.decode(z))\n",
        "\n",
        "    def reconstruct_img(self, x):\n",
        "        # encode image x\n",
        "        z_loc, z_scale = self.encode(x)\n",
        "        # sample in latent space\n",
        "        z = dist.Normal(z_loc, z_scale).sample()\n",
        "        # decode the image (note we don't sample in image space)\n",
        "        # loc_img = torch.sigmoid(self.decode(z))\n",
        "        return self.construct_from_z(z)\n",
        "\n",
        "vae = VAE()\n",
        "model_check = vae.model(img)\n",
        "\n",
        "z_loc, z_scale = vae.encode(img)\n",
        "out = vae.decode(nn.Flatten()(z))\n",
        "#  %%\n",
        "\n",
        "vae = VAE()\n",
        "model_check = vae.model(img)\n",
        "\n",
        "z_loc, z_scale = vae.encode(img)\n",
        "out = vae.decode(nn.Flatten()(z))\n",
        "\n",
        "# encode\n",
        "# guide_check = vae_model.guide(img)\n",
        "#  %%\n",
        "\n",
        "optimizer = Adam({\"lr\": 1.0e-3})\n",
        "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO())\n",
        "\n",
        "for x in dataloader:\n",
        "    epoch_loss = svi.step(x)\n",
        "    print(epoch_loss)\n",
        "    break\n",
        "#  %%\n",
        "# TODO better loss is needed, outshapes are currently not always full\n",
        "# loss_fn = torch.nn.MSELoss()\n",
        "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "class LitAutoEncoder(pl.LightningModule):\n",
        "    def __init__(self, batch_size=1, learning_rate=1e-3):\n",
        "        super().__init__()\n",
        "        self.autoencoder = AutoEncoder(batch_size, 1)\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss_fn = torch.nn.MSELoss()\n",
        "        self.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "        # self.vae = VAE()\n",
        "        # self.vae_flag = vae_flag\n",
        "        # self.loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.autoencoder(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        inputs = train_batch\n",
        "        output = self.forward(inputs)\n",
        "        loss = self.loss_fn(output, inputs)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        # tensorboard = self.logger.experiment\n",
        "        self.logger.experiment.add_scalar(\"Loss/train\", loss, batch_idx)\n",
        "        # torchvision.utils.make_grid(output)\n",
        "        self.logger.experiment.add_image(\n",
        "            \"input\", torchvision.utils.make_grid(inputs), batch_idx)\n",
        "        self.logger.experiment.add_image(\n",
        "            \"output\", torchvision.utils.make_grid(torch.sigmoid(output)), batch_idx)\n",
        "\n",
        "        # tensorboard.add_image(\"input\", transforms.ToPILImage()(output[batch_idx]), batch_idx)\n",
        "        # tensorboard.add_image(\"output\", transforms.ToPILImage()(output[batch_idx]), batch_idx)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class LitVariationalAutoEncoder(pl.LightningModule):\n",
        "    def __init__(self, batch_size=1, learning_rate=1e-3):\n",
        "        super().__init__()\n",
        "        # self.autoencoder = AutoEncoder(batch_size, 1)\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.vae = VAE()\n",
        "        # self.vae = VAE()\n",
        "        # self.vae_flag = vae_flag\n",
        "        self.loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.vae.forward(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def torch_training_step(self, train_batch, batch_idx):\n",
        "        inputs = train_batch\n",
        "        output = self.forward(inputs)\n",
        "        loss = self.loss_fn(output, inputs)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        # tensorboard = self.logger.experiment\n",
        "        self.logger.experiment.add_scalar(\"Loss/train\", loss, batch_idx)\n",
        "        # torchvision.utils.make_grid(output)\n",
        "        self.logger.experiment.add_image(\n",
        "            \"input\", torchvision.utils.make_grid(inputs), batch_idx)\n",
        "        self.logger.experiment.add_image(\n",
        "            \"output\", torchvision.utils.make_grid(torch.sigmoid(output)), batch_idx)\n",
        "\n",
        "    def pyro_training_step(self, train_batch, batch_idx):\n",
        "        inputs = train_batch\n",
        "        output = self.vae.reconstruct_img(inputs)\n",
        "        loss = self.loss_fn(self.vae.model, self.vae.guide, inputs)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        self.logger.experiment.add_scalar(\"Loss/train\", loss, batch_idx)\n",
        "        self.logger.experiment.add_image(\n",
        "            \"input\", torchvision.utils.make_grid(inputs), batch_idx)\n",
        "        self.logger.experiment.add_image(\n",
        "            \"output\", torchvision.utils.make_grid(torch.sigmoid(output)), batch_idx)\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        return self.torch_training_step(train_batch, batch_idx)\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        return self.pyro_training_step(train_batch, batch_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#  %%\n",
        "tb_logger = pl_loggers.TensorBoardLogger(\"runs/\")\n",
        "\n",
        "# from pathlib import Path\n",
        "# Path(\"checkpoints/\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "            dirpath=\"checkpoints/\",\n",
        "            )\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=tb_logger,\n",
        "    enable_checkpointing=True,\n",
        "    gpus=1,\n",
        "    accumulate_grad_batches=1,\n",
        "    callbacks=[checkpoint_callback],\n",
        "    min_epochs=50,\n",
        "    max_epochs=75,\n",
        ")  # .from_argparse_args(args)\n",
        "\n",
        "#\n",
        "# if __name__ = main:\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model = LitAutoEncoder()\n",
        "model = LitVariationalAutoEncoder()\n",
        "trainer.fit(model, dataloader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#  %%\n",
        "a = []\n",
        "for data in train_dataset:\n",
        "    a.append(model(transform(data)))\n",
        "#  %%\n",
        "fig,ax = plt.subplots(5,5,figsize=(10,14))\n",
        "\n",
        "for ax in ax.flat:\n",
        "    z_random = torch.normal(torch.zeros_like(z),torch.ones_like(z)/10)\n",
        "    generated_image = model.vae.construct_from_z(z_random)\n",
        "    ax.imshow(transforms.ToPILImage()(torch.round(generated_image[0])))\n",
        "plt.show()\n",
        "\n",
        "# loss_function = torch.nn.MSELoss()\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
